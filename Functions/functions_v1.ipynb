{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446304ba-57a9-440e-805d-80b0b14dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4ee6f-4ad2-43ed-b862-11dec4931247",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b41d101-18bc-4fca-b66f-d71fbf73a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_date(date_str): #converts date string to datetime format\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_str, \"%B %d %Y\")\n",
    "    except:\n",
    "        date_obj = datetime.strptime(date_str, \"%b %d %Y\")\n",
    "\n",
    "    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    return formatted_date\n",
    "\n",
    "def convert_current_date(date_str):\n",
    "    \n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%a %b %d\").replace(year=datetime.now().year).strftime(\"%Y-%m-%d\") \n",
    "    except:\n",
    "        return convert_to_date(date_str)\n",
    "    \n",
    "\n",
    "def split_space(str): #splits string based on ' ' character\n",
    "    num1, num2 = str.split(\" \")[0].strip(), str.split(\" \")[1].strip()\n",
    "    return pd.Series([num1, num2])\n",
    "\n",
    "def convert_string_to_int(str): #the scraper picks up non-ASCII characters, so we replace them\n",
    "    return int(str.replace('âˆ’', '-')) \n",
    "\n",
    "def result_list(score_1, score_2):\n",
    "    \n",
    "    if score_1 > score_2:\n",
    "        return [1,0,0]\n",
    "    elif score_1 == score_2:\n",
    "        return [0,1,0]\n",
    "    elif score_1 < score_2:\n",
    "        return [0,0,1]\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "\n",
    "def result_classification(score_1, score_2):\n",
    "    \n",
    "    if score_1 > score_2:\n",
    "        return \"T1\"\n",
    "    elif score_1 == score_2:\n",
    "        return \"D\"\n",
    "    elif score_1 < score_2:\n",
    "        return \"T2\"\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "def clean_raw_scraped_df(df_raw): #cleans the base df that we scraped from elorankings.com\n",
    "\n",
    "    df_clean = pd.DataFrame()\n",
    "    df_clean[\"date\"] = df_raw[\"date\"].apply(convert_to_date)\n",
    "    df_clean[[\"team_1\", \"team_2\"]] = df_raw[[\"team_1\", \"team_2\"]]\n",
    "    df_clean[[\"score_1\", \"score_2\"]] = df_raw[\"score\"].apply(lambda x: split_space(x))\n",
    "    df_clean[\"location\"] = df_raw[\"location\"]\n",
    "    df_clean[[\"elo_change_1\", \"elo_change_2\"]] = df_raw[\"change_1\"].apply(lambda x: split_space(x))\n",
    "    df_clean[[\"new_elo_1\", \"new_elo_2\"]] = df_raw[\"score_1_points\"].apply(lambda x: split_space(x))\n",
    "    df_clean[[\"rank_change_1\", \"rank_change_2\"]] = df_raw[\"change_2\"].apply(lambda x: split_space(x))\n",
    "    df_clean[[\"new_rank_1\", \"new_rank_2\"]] = df_raw[\"score_2_points\"].apply(lambda x: split_space(x))\n",
    "\n",
    "    df_clean[\"old_elo_1\"] = df_clean[\"new_elo_1\"].apply(int) - (df_clean[\"elo_change_1\"].apply(convert_string_to_int))\n",
    "    df_clean[\"old_elo_2\"] = df_clean[\"new_elo_2\"].apply(int) - (df_clean[\"elo_change_2\"].apply(convert_string_to_int))\n",
    "\n",
    "    df_clean[\"old_rank_1\"] = df_clean[\"new_rank_1\"].apply(int) - (df_clean[\"rank_change_1\"].apply(convert_string_to_int))\n",
    "    df_clean[\"old_rank_2\"] = df_clean[\"new_rank_2\"].apply(int) - (df_clean[\"rank_change_2\"].apply(convert_string_to_int))\n",
    "\n",
    "    df_clean[\"elo_diff_1\"] = df_clean[\"old_elo_1\"] - df_clean[\"old_elo_2\"]\n",
    "    df_clean[\"elo_diff_2\"] = -1*df_clean[\"elo_diff_1\"]\n",
    "\n",
    "    df_clean[\"result\"] = df_clean.apply(lambda row: result_list(row['score_1'], row['score_2']), axis=1)\n",
    "    df_clean[\"result_class\"] = df_clean.apply(lambda row: result_classification(row['score_1'], row['score_2']), axis=1)\n",
    "\n",
    "    df_clean[\"game_id\"] = df_clean[\"date\"].apply(str) + df_clean[\"team_1\"] + df_clean[\"team_2\"]\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def create_one_col_df(df): #transform df to have one team per column\n",
    "\n",
    "    l_1 = []\n",
    "    l_2 = []\n",
    "\n",
    "    for x in df.columns:\n",
    "\n",
    "        if (\"2\" in x) & (\"team\" not in x):\n",
    "            l_2.append(x)\n",
    "        elif (\"1\" in x) & (\"team\" not in x):\n",
    "            l_1.append(x)\n",
    "        else:\n",
    "            l_1.append(x)\n",
    "            l_2.append(x)\n",
    "\n",
    "\n",
    "    df_1 = df[l_1]\n",
    "    df_2 = df[l_2]\n",
    "\n",
    "    for y in df_2:\n",
    "        if \"_2\" in y:\n",
    "            df_2 = df_2.rename(columns={y: y.replace(\"_2\", \"\")})\n",
    "\n",
    "    df_2 = df_2.rename(columns={\"team_1\": \"opp\"})\n",
    "\n",
    "\n",
    "    for z in df_1:\n",
    "        if \"_1\" in z:\n",
    "            df_1 = df_1.rename(columns={z: z.replace(\"_1\", \"\")})\n",
    "\n",
    "    df_1 = df_1.rename(columns={\"team_2\": \"opp\"})\n",
    "\n",
    "    df_fin = pd.concat([df_1, df_2]).sort_values(by = \"game_id\").reset_index()[\n",
    "        [\"game_id\", \"date\", \"location\", \"team\", \"opp\", \"score\", \"old_elo\", \"elo_change\", \"new_elo\", \"old_rank\", \"rank_change\", \"new_rank\", \"elo_diff\"]\n",
    "    ]\n",
    "\n",
    "    return df_fin\n",
    "\n",
    "\n",
    "def get_opp_stats(df): #Used to get data on the opponent\n",
    "\n",
    "    df_opp = df[[\"game_id\", \"team\", \"score\", \"old_elo\", \"old_rank\"]].rename(columns = {\n",
    "\n",
    "        \"team\":\"opp\",\n",
    "        \"score\":\"opp_score\",\n",
    "        \"old_elo\":\"opp_elo\",\n",
    "        \"old_rank\":\"opp_rank\",\n",
    "    }).set_index(['game_id', 'opp'])\n",
    "\n",
    "\n",
    "    df_joined = df.join(df_opp, on = [\"game_id\", \"opp\"], how = \"left\")\n",
    "\n",
    "    df_joined[\"result_class\"] = df_joined.apply(lambda row: result_classification(row['score'], row['opp_score']), axis=1)\n",
    "\n",
    "\n",
    "    return df_joined[[\"game_id\", \"date\", \"location\", \"team\", \"opp\", \"score\", \"opp_score\", \"old_elo\", \"opp_elo\", \"elo_diff\", \"result_class\"]]\n",
    "\n",
    "\n",
    "def apply_elo_50_rounding(elo):\n",
    "    return elo - (elo % 50)\n",
    "\n",
    "\n",
    "\n",
    "#def get_opponent_data(df):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff74dd-bce3-492e-af12-a2d18e2c4c4d",
   "metadata": {},
   "source": [
    "# xG Models + Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc3673f-19f5-4dd4-8393-fe9a4e61858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xg_model(df, model_type, test_size = 0.2, splits = 1):\n",
    "\n",
    "    #create folds that are used to split based on game id to avoid spillage\n",
    "    #Eventually we probably can move to stratified kfolds \n",
    "    splitter = GroupShuffleSplit(test_size=test_size, random_state = 0, n_splits=splits)\n",
    "    split = splitter.split(df, groups=df['game_id'])\n",
    "    \n",
    "    model_list = []\n",
    "    validation_result = []\n",
    "    \n",
    "    #we will set it up as if we were training multiple folds, but for now just doing one fold\n",
    "    for train_inds, test_inds in split:\n",
    "        train = df.iloc[train_inds]\n",
    "        test = df.iloc[test_inds]\n",
    "    \n",
    "        #Currently model uses a teams elo, their opposition elo, and the elo difference\n",
    "        \n",
    "        x_train = train[[\"old_elo\", \"opp_elo\", \"elo_diff\"]]\n",
    "        y_train = (train[\"score\"])\n",
    "        \n",
    "        x_test = test[[ \"old_elo\", \"opp_elo\", \"elo_diff\"]]\n",
    "        y_test = (test[\"score\"])\n",
    "\n",
    "        if model_type == \"GBM\":\n",
    "            model = GradientBoostingRegressor(max_depth = 10, n_estimators = 1000, random_state=0) #initialize the model\n",
    "\n",
    "        #We might have an error with multicollinearity with the lin regression since elo diff = old_elo - opp_elo, but I dont remember\n",
    "        #when that becomes a real issue\n",
    "        elif model_type == \"linreg\":\n",
    "            model = LinearRegression()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Model_type not supported\")\n",
    "        \n",
    "        model.fit(x_train, y_train) #big fit\n",
    "        model_list.append(model) #Save model down\n",
    "\n",
    "        #Get our validation set predictions\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        #Calculate rmse for goals scored\n",
    "        rmse = math.sqrt(mean_squared_error(y_test, y_pred ))\n",
    "        \n",
    "        validation_result.append(rmse)\n",
    "\n",
    "    return model_list, validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0d6224-dc01-4632-b02b-c36ad2348ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_xgboost_model(df, model_type, test_size = 0.2, splits = 1):\n",
    "\n",
    "    #create folds that are used to split based on game id to avoid spillage\n",
    "    #Eventually we probably can move to stratified kfolds \n",
    "    splitter = GroupShuffleSplit(test_size=test_size, random_state = 0, n_splits=splits)\n",
    "    split = splitter.split(df, groups=df['game_id'])\n",
    "    \n",
    "    model_list = []\n",
    "    validation_result = []\n",
    "    \n",
    "    #we will set it up as if we were training multiple folds, but for now just doing one fold\n",
    "    for train_inds, test_inds in split:\n",
    "        train = df.iloc[train_inds]\n",
    "        test = df.iloc[test_inds]\n",
    "\n",
    "        x_train = train[[\"old_elo\", \"opp_elo\", \"elo_diff\"]]\n",
    "        y_train = (train[\"result\"])\n",
    "        \n",
    "        x_test = test[[ \"old_elo\", \"opp_elo\", \"elo_diff\"]]\n",
    "        y_test = (test[\"result\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbecf1e-d14e-445b-be80-ec910f465d8b",
   "metadata": {},
   "source": [
    "# Elo Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5407bdd-e16f-47f1-8183-606a036da574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_elo(elo_diff, joined):\n",
    "\n",
    "    elo_rounded = apply_elo_50_rounding(int(elo_diff))\n",
    "    return joined[joined[\"rounded_elo\"] == elo_rounded].sort_values(\"result\")[\"pct\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462017ff-56a0-45e0-b560-adeff2709be0",
   "metadata": {},
   "source": [
    "# Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cc43e0-fea3-4323-8aeb-bfcb7535afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given two xgs, simulates scores of each side using a poisson distribution and then sees which side won / loss\n",
    "def possion_sim(xg, opp_xg, num):\n",
    "    \n",
    "    win  = 0\n",
    "    draw = 0\n",
    "    loss = 0\n",
    "    \n",
    "    if xg <= 0:\n",
    "        xg = 0.1\n",
    "    \n",
    "    if opp_xg <= 0:\n",
    "        opp_xg = 0.1\n",
    "    \n",
    "    for x in range(0, num):\n",
    "        team_goals = np.random.poisson(xg)\n",
    "        opp_goals = np.random.poisson(opp_xg)\n",
    "        \n",
    "        if team_goals > opp_goals:\n",
    "            win += 1\n",
    "        elif team_goals == opp_goals:\n",
    "            draw+=1\n",
    "        else:\n",
    "            loss+=1\n",
    "    \n",
    "    return [win / num, draw / num, loss / num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf85911-d9da-4446-ae8d-f85cf871a707",
   "metadata": {},
   "source": [
    "# Accuracy Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e796534b-02dc-4d2a-b82a-958541e3b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://en.wikipedia.org/wiki/Brier_score\n",
    "def single_brier_score(list_pred, list_outcome):\n",
    "    \n",
    "    cum = 0\n",
    "    for x in range(0, len(list_pred)):\n",
    "        cum+= (list_pred[x] - list_outcome[x])**2\n",
    "    \n",
    "    return cum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
